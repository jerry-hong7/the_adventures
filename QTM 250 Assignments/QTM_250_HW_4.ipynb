{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework #4: **Facial Expression Recognition Experiment in ML**\n",
        "\n",
        "\n",
        "### Group Members: Faiz Daredia, Conor Harty, Catherine Haus, Jerry Hong, & Lenora Upchurch"
      ],
      "metadata": {
        "id": "QAB-Heugi_qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Introduction**:\n",
        "\n",
        "Facial recognition software has flourished in recent years. Emotion analysis is a popular feature of this software, and it has a variety of applications.The worldwide emotion detection and recognition market is expected to almost double in only five years from 23.5 billion dollars in 2022 to 42.9 billion in 2027. Companies are using it for a variety of purposes including interviewing candidates. For example, Unilever has used it to detect the confidence level of candidates and decide whether they can take on client-facing roles. Kellogg has also used the technology to assess the emotional reactions to its advertisements and help them decide which ones are the most effective. However, its usage does not end there. The technology has been used by medical professionals to identify medical disorders, pain, and the feelings of those who cannot properly communicate their emotions including autistic or depressed individuals.  \n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/catherinehaus/QTM250-example/blob/main/sad.jpg?raw=true\" width=\"300\">\n",
        "<img src=\"https://github.com/catherinehaus/QTM250-example/blob/main/smile.jpeg?raw=true\" width=\"200\">\n",
        "</center>\n",
        "\n",
        "1) A variety of states have implemented intelligent emotion analysis into their police programs for public safety. \"In particular, intelligent emotion analysis permeates such applications as:\n",
        "- lie detectors and smart border solutions to capture high-quality biometric data;\n",
        "- interpreting and predicting potential terrorism threats in public spaces;\n",
        "- crime scene investigation to analyze potential motives in a crime\" (Isakova 2022).  \n",
        "\n",
        "Other uses of intelligent emotion analysis include:\n",
        "\n",
        "2) customer behavior analysis (for marketing purposes),\n",
        "\n",
        "3) video game testing (to gauge players' reactions to a new game)\n",
        "\n",
        "4) healthcare.\n",
        "\n",
        "We are not particularly interested in the software's application to public safety. Numerous published studies have found that facial regonition softwares incorrectly predict who is most likely to commit crimes in the future. These models also have issues of bias and racism when applied to crime predictions.\n",
        "\n",
        "We are most interested in the application of intelligent emotion analysis to the healthcare industry. Smart healthcare centers with facial regonition software can experience a variety of benefits (and provide benefits to patients):\n",
        "- detect depression and start treatment earlier;\n",
        "- detect degenerative nerve diseases earlier;\n",
        "- prevent suicide more effectively;\n",
        "- monitor patients during treatment to gauge responses (Isakova).\n",
        "\n",
        "Given that emotion detection technology is expecting to grow rapidly over the next decade and is being deployed across a variety of industries, it is especially important to understand its efficacy. Through this project, we are using both stock photos and photos we generated to understand if emotion detection technology struggles identifying some emotions over others, espeiclaly those that are more nuanced such as indifference, guilt, and annoyance.\n",
        "\n",
        "                                            \n",
        "                                            \n",
        "Work Cited:\n",
        "\n",
        "Isakova, Tatsiana. \"Benefits of AI facial expression recognition,\" InData\n",
        "Labs, 11 October 2022, https://indatalabs.com/blog/ai-facial-expression-recognition. Accessed 11 April 2023.\n"
      ],
      "metadata": {
        "id": "s8tCzTTkjs1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cloud Vision API Overview**\n",
        "\n",
        "Google Cloud promotes its Vision API service as a tool developers can use to detect certain features in applications. A few of Google Cloud Vision's uses include landmark detection, optical character recognition (OCR), and image labeling. As the popularity of the **Cloud Vision API** increases, we will explore the accuracy of the service by sending images to the Vision API and seeing how it detects image properties, such as joy, sorrow, or anger."
      ],
      "metadata": {
        "id": "X0hskd2sfUPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Data Collection**\n",
        "\n",
        "In order to investigate the performance and accuracy of the Cloud Vision API, we identified five emotions to test the detection service on. The team's emotions include the following:\n",
        "\n",
        "1.   Pensive\n",
        "2.   Annoyed\n",
        "3.   Embarassed\n",
        "4.   Relaxed\n",
        "5.   Surprised\n",
        "\n",
        "The face detection feature has bounding boxes for content sent through the algorithm, as well as confidence ratings for the properties of faces. Moreover, the response should have landmarks detected, like eyes and nose, with a corresponding confidence rating, like joy and surprise. Exploring the machine learning API, we wanted to test each emotion using stock images tagged with the appropriate emotion. The API outputs the confidence of the landmarks as well as the likelihood of different emotions. Since we hypothesized that Vision would be able to detect emotions in stock images easily, we chose one lab member to recreate her version of the chosen emotions, guessing that Lenora's emotions would be harder to detect through the API.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oFabKZG6vNQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pensive\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/large.jpg?raw=true\" width=\"500\">\n",
        "\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/image10.jpg?raw=true\" width=\"400\">\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "uVVpVWf5xHuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install jq"
      ],
      "metadata": {
        "id": "slZ47xu-Mc1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lenorapensive.json\n",
        "{\n",
        "  \"requests\": [\n",
        "      {\n",
        "        \"image\": {\n",
        "          \"source\": {\n",
        "              \"gcsImageUri\": \"gs://qtm250hw/lenorapensive.jpeg\"\n",
        "          }\n",
        "        },\n",
        "        \"features\": [\n",
        "          {\n",
        "            \"type\": \"FACE_DETECTION\",\n",
        "            \"maxResults\": 10\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "043m6zEv4vRC",
        "outputId": "e0b80c5f-d001-4006-a70b-6f152a7bb7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing lenorapensive.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export API_KEY=\"AIzaSyAzocleaQREaU73xr7Z66TxaiIqCWZFhGY\"\n",
        "curl -s -X POST -H \"Content-Type: application/json\" --data-binary @lenorapensive.json  https://vision.googleapis.com/v1/images:annotate?key=${API_KEY}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt6wce-O43Eb",
        "outputId": "3507118b-ccbd-4170-ae88-27020a72dc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"responses\": [\n",
            "    {\n",
            "      \"faceAnnotations\": [\n",
            "        {\n",
            "          \"boundingPoly\": {\n",
            "            \"vertices\": [\n",
            "              {\n",
            "                \"x\": 450,\n",
            "                \"y\": 505\n",
            "              },\n",
            "              {\n",
            "                \"x\": 1040,\n",
            "                \"y\": 505\n",
            "              },\n",
            "              {\n",
            "                \"x\": 1040,\n",
            "                \"y\": 1191\n",
            "              },\n",
            "              {\n",
            "                \"x\": 450,\n",
            "                \"y\": 1191\n",
            "              }\n",
            "            ]\n",
            "          },\n",
            "          \"fdBoundingPoly\": {\n",
            "            \"vertices\": [\n",
            "              {\n",
            "                \"x\": 511,\n",
            "                \"y\": 666\n",
            "              },\n",
            "              {\n",
            "                \"x\": 984,\n",
            "                \"y\": 666\n",
            "              },\n",
            "              {\n",
            "                \"x\": 984,\n",
            "                \"y\": 1139\n",
            "              },\n",
            "              {\n",
            "                \"x\": 511,\n",
            "                \"y\": 1139\n",
            "              }\n",
            "            ]\n",
            "          },\n",
            "          \"landmarks\": [\n",
            "            {\n",
            "              \"type\": \"LEFT_EYE\",\n",
            "              \"position\": {\n",
            "                \"x\": 678.9443,\n",
            "                \"y\": 845.28613,\n",
            "                \"z\": 0.00031995773\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_EYE\",\n",
            "              \"position\": {\n",
            "                \"x\": 851.289,\n",
            "                \"y\": 822.4884,\n",
            "                \"z\": 31.885942\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LEFT_OF_LEFT_EYEBROW\",\n",
            "              \"position\": {\n",
            "                \"x\": 609.17456,\n",
            "                \"y\": 807.33936,\n",
            "                \"z\": 3.2068431\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_OF_LEFT_EYEBROW\",\n",
            "              \"position\": {\n",
            "                \"x\": 723.08,\n",
            "                \"y\": 794.8885,\n",
            "                \"z\": -26.589846\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LEFT_OF_RIGHT_EYEBROW\",\n",
            "              \"position\": {\n",
            "                \"x\": 809.87616,\n",
            "                \"y\": 784.60156,\n",
            "                \"z\": -11.014461\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_OF_RIGHT_EYEBROW\",\n",
            "              \"position\": {\n",
            "                \"x\": 903.17114,\n",
            "                \"y\": 775.3936,\n",
            "                \"z\": 56.79013\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"MIDPOINT_BETWEEN_EYES\",\n",
            "              \"position\": {\n",
            "                \"x\": 773.0569,\n",
            "                \"y\": 828.67004,\n",
            "                \"z\": -20.06027\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"NOSE_TIP\",\n",
            "              \"position\": {\n",
            "                \"x\": 791.5599,\n",
            "                \"y\": 938.04016,\n",
            "                \"z\": -67.59528\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"UPPER_LIP\",\n",
            "              \"position\": {\n",
            "                \"x\": 792.67706,\n",
            "                \"y\": 1007.2633,\n",
            "                \"z\": -25.872963\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LOWER_LIP\",\n",
            "              \"position\": {\n",
            "                \"x\": 795.8485,\n",
            "                \"y\": 1059.2141,\n",
            "                \"z\": -11.685377\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"MOUTH_LEFT\",\n",
            "              \"position\": {\n",
            "                \"x\": 718.7579,\n",
            "                \"y\": 1034.3228,\n",
            "                \"z\": 8.27916\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"MOUTH_RIGHT\",\n",
            "              \"position\": {\n",
            "                \"x\": 854.1187,\n",
            "                \"y\": 1016.9573,\n",
            "                \"z\": 33.828926\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"MOUTH_CENTER\",\n",
            "              \"position\": {\n",
            "                \"x\": 793.54083,\n",
            "                \"y\": 1026.776,\n",
            "                \"z\": -12.018048\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"NOSE_BOTTOM_RIGHT\",\n",
            "              \"position\": {\n",
            "                \"x\": 825.5787,\n",
            "                \"y\": 948.68195,\n",
            "                \"z\": 6.038902\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"NOSE_BOTTOM_LEFT\",\n",
            "              \"position\": {\n",
            "                \"x\": 737.4154,\n",
            "                \"y\": 959.8733,\n",
            "                \"z\": -11.435358\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"NOSE_BOTTOM_CENTER\",\n",
            "              \"position\": {\n",
            "                \"x\": 788.5462,\n",
            "                \"y\": 968.39636,\n",
            "                \"z\": -27.863876\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LEFT_EYE_TOP_BOUNDARY\",\n",
            "              \"position\": {\n",
            "                \"x\": 675.7468,\n",
            "                \"y\": 826.8812,\n",
            "                \"z\": -10.878106\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LEFT_EYE_RIGHT_CORNER\",\n",
            "              \"position\": {\n",
            "                \"x\": 718.6269,\n",
            "                \"y\": 843.85034,\n",
            "                \"z\": 6.7112856\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LEFT_EYE_BOTTOM_BOUNDARY\",\n",
            "              \"position\": {\n",
            "                \"x\": 679.8672,\n",
            "                \"y\": 859.5351,\n",
            "                \"z\": -1.6996675\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LEFT_EYE_LEFT_CORNER\",\n",
            "              \"position\": {\n",
            "                \"x\": 641.73615,\n",
            "                \"y\": 852.9578,\n",
            "                \"z\": 9.11133\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_EYE_TOP_BOUNDARY\",\n",
            "              \"position\": {\n",
            "                \"x\": 852.9818,\n",
            "                \"y\": 804.37354,\n",
            "                \"z\": 21.815498\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_EYE_RIGHT_CORNER\",\n",
            "              \"position\": {\n",
            "                \"x\": 886.652,\n",
            "                \"y\": 817.9207,\n",
            "                \"z\": 54.53702\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_EYE_BOTTOM_BOUNDARY\",\n",
            "              \"position\": {\n",
            "                \"x\": 854.8594,\n",
            "                \"y\": 835.32837,\n",
            "                \"z\": 30.87362\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_EYE_LEFT_CORNER\",\n",
            "              \"position\": {\n",
            "                \"x\": 816.4225,\n",
            "                \"y\": 830.1248,\n",
            "                \"z\": 25.29395\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LEFT_EYEBROW_UPPER_MIDPOINT\",\n",
            "              \"position\": {\n",
            "                \"x\": 665.295,\n",
            "                \"y\": 785.04565,\n",
            "                \"z\": -23.241913\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_EYEBROW_UPPER_MIDPOINT\",\n",
            "              \"position\": {\n",
            "                \"x\": 856.92426,\n",
            "                \"y\": 762.94916,\n",
            "                \"z\": 11.500702\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LEFT_EAR_TRAGION\",\n",
            "              \"position\": {\n",
            "                \"x\": 549.83496,\n",
            "                \"y\": 932.2195,\n",
            "                \"z\": 198.4253\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_EAR_TRAGION\",\n",
            "              \"position\": {\n",
            "                \"x\": 921.79254,\n",
            "                \"y\": 858.4346,\n",
            "                \"z\": 272.14038\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"FOREHEAD_GLABELLA\",\n",
            "              \"position\": {\n",
            "                \"x\": 768.3476,\n",
            "                \"y\": 790.9858,\n",
            "                \"z\": -24.932318\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"CHIN_GNATHION\",\n",
            "              \"position\": {\n",
            "                \"x\": 800.8433,\n",
            "                \"y\": 1134.4583,\n",
            "                \"z\": 16.364351\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"CHIN_LEFT_GONION\",\n",
            "              \"position\": {\n",
            "                \"x\": 610.81494,\n",
            "                \"y\": 1068.5596,\n",
            "                \"z\": 138.08669\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"CHIN_RIGHT_GONION\",\n",
            "              \"position\": {\n",
            "                \"x\": 906.0718,\n",
            "                \"y\": 1029.0898,\n",
            "                \"z\": 199.55142\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"LEFT_CHEEK_CENTER\",\n",
            "              \"position\": {\n",
            "                \"x\": 657.85583,\n",
            "                \"y\": 969.2279,\n",
            "                \"z\": 14.191502\n",
            "              }\n",
            "            },\n",
            "            {\n",
            "              \"type\": \"RIGHT_CHEEK_CENTER\",\n",
            "              \"position\": {\n",
            "                \"x\": 894.3966,\n",
            "                \"y\": 935.98883,\n",
            "                \"z\": 58.19698\n",
            "              }\n",
            "            }\n",
            "          ],\n",
            "          \"rollAngle\": -6.4275584,\n",
            "          \"panAngle\": 10.167812,\n",
            "          \"tiltAngle\": -2.844058,\n",
            "          \"detectionConfidence\": 0.9921875,\n",
            "          \"landmarkingConfidence\": 0.6650691,\n",
            "          \"joyLikelihood\": \"UNLIKELY\",\n",
            "          \"sorrowLikelihood\": \"VERY_UNLIKELY\",\n",
            "          \"angerLikelihood\": \"VERY_UNLIKELY\",\n",
            "          \"surpriseLikelihood\": \"VERY_UNLIKELY\",\n",
            "          \"underExposedLikelihood\": \"VERY_UNLIKELY\",\n",
            "          \"blurredLikelihood\": \"VERY_UNLIKELY\",\n",
            "          \"headwearLikelihood\": \"VERY_UNLIKELY\"\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annoyed\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/360_F_229232603_fj4A81mwpkwBqng2jVqytLtuACZwUkO0.jpg?raw=true\" width=\"500\">\n",
        "\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/image4.jpg?raw=true\" width=\"400\">\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "oR0ZUt-rxWSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lenoraannoyed.json\n",
        "{\n",
        "  \"requests\": [\n",
        "      {\n",
        "        \"image\": {\n",
        "          \"source\": {\n",
        "              \"gcsImageUri\": \"gs://qtm250hw/lenoraannoyed.jpeg\"\n",
        "          }\n",
        "        },\n",
        "        \"features\": [\n",
        "          {\n",
        "            \"type\": \"FACE_DETECTION\",\n",
        "            \"maxResults\": 10\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAhUlW5149RT",
        "outputId": "38381f35-efdb-42ef-e92e-035f9c9c8d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing lenoraannoyed.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export API_KEY=\"AIzaSyAzocleaQREaU73xr7Z66TxaiIqCWZFhGY\"\n",
        "curl -s -X POST -H \"Content-Type: application/json\" --data-binary @lenoraannoyed.json  https://vision.googleapis.com/v1/images:annotate?key=${API_KEY}"
      ],
      "metadata": {
        "id": "uQR-x9uf5Fjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embarassed\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/Embarrassed-Shutterstock-Post.jpg?raw=true\" width=\"500\">\n",
        "\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/image1.jpg?raw=true\" width=\"400\">\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "zQlpYKF3xaXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lenoraembarrassed.json\n",
        "{\n",
        "  \"requests\": [\n",
        "      {\n",
        "        \"image\": {\n",
        "          \"source\": {\n",
        "              \"gcsImageUri\": \"gs://qtm250hw/lenoraembarrassed.jpeg\"\n",
        "          }\n",
        "        },\n",
        "        \"features\": [\n",
        "          {\n",
        "            \"type\": \"FACE_DETECTION\",\n",
        "            \"maxResults\": 10\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "kyyzF-EK5T_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export API_KEY=\"AIzaSyAzocleaQREaU73xr7Z66TxaiIqCWZFhGY\"\n",
        "curl -s -X POST -H \"Content-Type: application/json\" --data-binary @lenoraembarrassed.json  https://vision.googleapis.com/v1/images:annotate?key=${API_KEY}"
      ],
      "metadata": {
        "id": "uNwXkOGv5aKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relaxed\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/hero-older-woman-relaxing-2000x1333.jpg?raw=true\" width=\"500\">\n",
        "\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/image7.jpg?raw=true\" width=\"400\">\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "FrCtgBH2xfTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lenorarelaxed.json\n",
        "{\n",
        "  \"requests\": [\n",
        "      {\n",
        "        \"image\": {\n",
        "          \"source\": {\n",
        "              \"gcsImageUri\": \"gs://qtm250hw/lenorarelaxed.jpeg\"\n",
        "          }\n",
        "        },\n",
        "        \"features\": [\n",
        "          {\n",
        "            \"type\": \"FACE_DETECTION\",\n",
        "            \"maxResults\": 10\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "3pfqXWTm5j7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export API_KEY=\"AIzaSyAzocleaQREaU73xr7Z66TxaiIqCWZFhGY\"\n",
        "curl -s -X POST -H \"Content-Type: application/json\" --data-binary @lenorarelaxed.json  https://vision.googleapis.com/v1/images:annotate?key=${API_KEY}"
      ],
      "metadata": {
        "id": "020izOO_5sfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Surprised\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/istockphoto-153177844-170667a.jpg?raw=true\">\n",
        "\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/image8.jpg?raw=true\" width=\"400\">\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "dkHKGqg_x56d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lenorasurprised.json\n",
        "{\n",
        "  \"requests\": [\n",
        "      {\n",
        "        \"image\": {\n",
        "          \"source\": {\n",
        "              \"gcsImageUri\": \"gs://qtm250hw/lenorasurprised.jpeg\"\n",
        "          }\n",
        "        },\n",
        "        \"features\": [\n",
        "          {\n",
        "            \"type\": \"FACE_DETECTION\",\n",
        "            \"maxResults\": 10\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO_d7GAufMKp",
        "outputId": "95882444-390d-4741-d1c6-5c20daac9bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing lenorasurprised.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export API_KEY=\"AIzaSyAzocleaQREaU73xr7Z66TxaiIqCWZFhGY\"\n",
        "curl -s -X POST -H \"Content-Type: application/json\" --data-binary @lenorasurprised.json  https://vision.googleapis.com/v1/images:annotate?key=${API_KEY}"
      ],
      "metadata": {
        "id": "76t8nT5nfm7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "APIKEY = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uddgJhs5BHjk",
        "outputId": "6c8ddd40-07cd-4cd4-ee38-9d6bc9b00b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install jq"
      ],
      "metadata": {
        "id": "vDUvQqRsxP1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pandas"
      ],
      "metadata": {
        "id": "ofokSzJyQTtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "okcvUPdJQin_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "SjntV6t1TJ5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = gc.open_by_url('https://docs.google.com/spreadsheets/d/16DbHxFLnWa0U0qtglkhs4QeSy9CPo8B5Tjpb4S91deY/edit#gid=0')\n",
        "sheet = sp.worksheet('Sheet1')\n",
        "data = sheet.get_all_values()\n",
        "df = pd.DataFrame(data)\n",
        "df.columns = df.iloc[0]\n",
        "df = df.iloc[1:]\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "3IMFgV_UhmvU",
        "outputId": "f75939cb-1169-407e-9bd6-59697ed1d3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      Emotion detectionConfidence landmarkingConfidence  joyLikelihood  \\\n",
              "1      Pensive           0.9921875             0.6650691       UNLIKELY   \n",
              "2      Annoyed           0.9921875            0.76419157  VERY_UNLIKELY   \n",
              "3  Embarrassed          0.99609375             0.7663256  VERY_UNLIKELY   \n",
              "4      Relaxed           0.9921875            0.73436284    VERY_LIKELY   \n",
              "5    Surprised            0.984375             0.3930697  VERY_UNLIKELY   \n",
              "\n",
              "0 sorrowLikelihood angerLikelihood surpriseLikelihood  \n",
              "1    VERY_UNLIKELY   VERY_UNLIKELY      VERY_UNLIKELY  \n",
              "2         UNLIKELY   VERY_UNLIKELY      VERY_UNLIKELY  \n",
              "3      VERY_LIKELY        UNLIKELY      VERY_UNLIKELY  \n",
              "4    VERY_UNLIKELY   VERY_UNLIKELY      VERY_UNLIKELY  \n",
              "5         POSSIBLE          LIKELY        VERY_LIKELY  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1dd8bd9-8e09-4ac5-9ee9-3b9e281ae9c5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>detectionConfidence</th>\n",
              "      <th>landmarkingConfidence</th>\n",
              "      <th>joyLikelihood</th>\n",
              "      <th>sorrowLikelihood</th>\n",
              "      <th>angerLikelihood</th>\n",
              "      <th>surpriseLikelihood</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pensive</td>\n",
              "      <td>0.9921875</td>\n",
              "      <td>0.6650691</td>\n",
              "      <td>UNLIKELY</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Annoyed</td>\n",
              "      <td>0.9921875</td>\n",
              "      <td>0.76419157</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "      <td>UNLIKELY</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Embarrassed</td>\n",
              "      <td>0.99609375</td>\n",
              "      <td>0.7663256</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "      <td>VERY_LIKELY</td>\n",
              "      <td>UNLIKELY</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Relaxed</td>\n",
              "      <td>0.9921875</td>\n",
              "      <td>0.73436284</td>\n",
              "      <td>VERY_LIKELY</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Surprised</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.3930697</td>\n",
              "      <td>VERY_UNLIKELY</td>\n",
              "      <td>POSSIBLE</td>\n",
              "      <td>LIKELY</td>\n",
              "      <td>VERY_LIKELY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1dd8bd9-8e09-4ac5-9ee9-3b9e281ae9c5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1dd8bd9-8e09-4ac5-9ee9-3b9e281ae9c5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1dd8bd9-8e09-4ac5-9ee9-3b9e281ae9c5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Architecture Diagram**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/QTM%20250%20HW%204.drawio.png?raw=true\" width=\"900\">"
      ],
      "metadata": {
        "id": "kSwQzIBBzy_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Results & Discussion**\n",
        "Now, we want to analyze the accuracy of the Vision API results. We will use a spreadsheet to compare the output confidence levels and the likelihood of various tested emotions that the Vision API provided. Under the accuracy column.\n",
        "\n",
        "Links to team's GitHub:\n",
        "\n",
        "https://github.com/lupchur3/QTM-250\n",
        "\n",
        "Here are our **findings**:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/16DbHxFLnWa0U0qtglkhs4QeSy9CPo8B5Tjpb4S91deY/edit?usp=sharing"
      ],
      "metadata": {
        "id": "8re2hHWGuMRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://github.com/lupchur3/QTM-250/blob/main/Detection%20and%20Landmark%20Confidence%20by%20Emotion%20(1).png?raw=true\">\n",
        "\n"
      ],
      "metadata": {
        "id": "BFsdabDpjVty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technologies built to recognize emotions have developed and grown through the years, and, as seen with the Google Cloud Vision API, software does have the capacity to detect a variety of emotions using advanced image dispensation. This “artificial Intelligence” detects and studies different facial expressions based on whatever training data was classified. As we have discovered, however, emotion recognition, like Vision, is not perfect, nor will it ever be. One challenge is that datasets are labeled by people. What one perceives as a happy person may in fact be someone feeling sad or depressed.\n",
        "\n",
        "In summary, **Google Cloud Vision API** is a useful tool if web, label, or face detection is needed. However, the model's ability to identify specific emotions are not reliably accurate. Although more time and resources would be needed to gain a broader perspective on the flaws of the ML model, the brief testing done by our team conveys the importance of  human-to-human interaction in certain fields, as depictions of emotion are inconsistent between individuals."
      ],
      "metadata": {
        "id": "qdq5yKP0vCTg"
      }
    }
  ]
}