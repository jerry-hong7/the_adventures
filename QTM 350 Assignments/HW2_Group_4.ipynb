{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1RY0H-mihWExKxYMlxJIBa6wtLqkstUfh","timestamp":1696262387067},{"file_id":"1YWBmRTF-ptk_kZYAB084HwzKNjPz-IA6","timestamp":1695740307519},{"file_id":"1HGTKfBrrgVU19P3RwpJct3QrO5_JiMo7","timestamp":1632236988192},{"file_id":"1-VKiu8sorw7Dp47vU_1fXBReM8CZS1eL","timestamp":1600110737989},{"file_id":"1CxZgRf0WBUPQ5yyBR0O433-W3oOqcSHu","timestamp":1598645009795}],"collapsed_sections":["RqhpdagcQ28W","Abzc6B2Z7ReJ","j5JvWysP7ReO","zLhOktrd7ReP","-t_VQwBc7ReQ","QZfCfM9s7ReR","vJgTwWRs55jb","_SWHMZ1X6SZS","Wqh1W1lZFCsd","Ea6hPztESLpK"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rvObaKKCzGwK"},"source":["<p>QTM 350 - Fall 2023 - Dr. Jeremy Jacobson</p>\n","<hr>\n","<center>\n","<h1>HOMEWORK 2</h1>\n","<p>Group 4: Jerry Hong, Eric Juarez, Jin Lee, <br>Christopher Roebuck, Raz Wachtel</p>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"RqhpdagcQ28W"},"source":["# Question 1\n","## NYTimes Bestseller - Book 4\n","\n"]},{"cell_type":"markdown","source":["To practice using APIs, we'll be gathering data from the NYTimes Bestseller List. First, we will need to use a Python package called `getpass`, so our API key is not exposed."],"metadata":{"id":"0IH13b3y0eGm"}},{"cell_type":"code","source":["import getpass\n","APIKEY = getpass.getpass()"],"metadata":{"id":"zJPCk9Xt0dQU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will start by making a GET request for the bestseller list."],"metadata":{"id":"0I1NIRSL2NKS"}},{"cell_type":"code","source":["!curl --request GET -o bestsellers.json \"https://api.nytimes.com/svc/books/v3/lists/current/hardcover-fiction.json?api-key=$APIKEY\""],"metadata":{"id":"24zrmhuw3Nuk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have the data in a `.json` format, we can transform it into a pandas dataframe."],"metadata":{"id":"n8Nir8nT6Dl5"}},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_json('bestsellers.json')"],"metadata":{"id":"azsjlFHn3nvk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We're interested in the fourth book on the bestseller list, so we'll index at position 3 and access the title."],"metadata":{"id":"6C7b8j4u-I_x"}},{"cell_type":"code","source":["df['results']['books'][3]['title']"],"metadata":{"id":"0qgC3xmR86Fr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we can find information like the book's cover, plot description, Amazon URL, and so much more."],"metadata":{"id":"rRc-ca2__-2w"}},{"cell_type":"code","source":["df['results']['books'][3]['book_image']"],"metadata":{"id":"ArRxLZbH_A5-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![Tom Lake cover photo](https://storage.googleapis.com/du-prd/books/images/9780063327528.jpg)"],"metadata":{"id":"clrmT5x_CRQp"}},{"cell_type":"code","source":["url = df['results']['books'][3]['amazon_product_url']\n","url"],"metadata":{"id":"oyPijohu_Trn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I wasn't able to use `requests` to scrape the price, but if you open the URL, you'll see that the hardcover version of *Tom Lake* is $17.99."],"metadata":{"id":"9657UWZqGbSa"}},{"cell_type":"code","source":["w = df['results']['books'][3]['weeks_on_list']\n","print(f'Tom Lake has been on the best seller list for {w} weeks.')"],"metadata":{"id":"4CN9jVwLFJUt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMHPjghx-w9f"},"source":["# Question 2\n","## Reading the classics\n","Among your group members, discuss the classics, e.g. books and authors found on Project Gutenberg, that you had read and loved, or hated. See Notebook 4 from the text for the link to this site as well as more details. Include your discussion in your notebook, along with pictures or links to the books to give the reader a sense of the books.\n","\n","Create three (or more) metrics that could measure what it was that you loved or hated about the books. Code the metric as a Python function. Explain each metric, including why you think it is a useful or interesting way to measure a book. For instance, a count of unique words could be a metric to measure vocabulary. Average sentence length could be a measure of the author's style. The frequency and distribution of the length of words could provide insights into the use of 'big' words by the author.\n","\n","Then, use `wget` to obtain the text of the books you discussed from Project Gutenberg, implement the metrics you created in Python, and analyze the books using these metrics.\n","\n","Hypothesize what you expect to find in the books before the analysis, and then discuss what you actually found. As much as possible, narrate and explain the insights provided by your metrics when appropriate.  If you are unsure how to get started, begin with the code the textbook used to analyze War and Peace, make simple modifications to it, justifying at each step what effect you expect your modification to have, and the actual impact it did have. For instance, instead of looking at a count of unique words, look at the max or min of this count.  "]},{"cell_type":"markdown","source":["## Our Books of Choice\n","\n","The books we have chosen are a mix of those we either loved or hated. This can give us a comparative analysis using our metrics of what makes us to love or hate the books of choice.\n","\n","[Romeo and Juliet Review]\n","\n","<img src=https://d28hgpri8am2if.cloudfront.net/book_images/cvr9781451621709_9781451621709_hr.jpg width=150 height=250>\n","\n","[Romeo and Juliet Review]:https://www.goodreads.com/book/show/18135.Romeo_and_Juliet?ref=nav_sb_ss_1_16\n","\n","Among us, who did read Romeo and Juliet did not like the book since the story can be seen as teenage drama that led to unnecessary tragedy. The use of unfamiliar old English language as Shakespeare writes in made it hard to follow the plot, and additionally the book's play style of writing led to choppy narratives, making it a harder read.\n","\n","[Frankenstein Review]\n","\n","<img src=https://d28hgpri8am2if.cloudfront.net/book_images/onix/cvr9781982146160/frankenstein-9781982146160_xlg.jpg width=150 height=250>\n","\n","[Frankenstein Review]: https://www.goodreads.com/book/show/35031085-frankenstein\n","\n","There was a mix of like and dislike for Frankenstein. Some liked the book for its unique story of creating a monster out of corpses and the ethical dilemmas that arise while others disliked it for its depressiveness and scholarly writing as Frankenstein is a student and the book has portions written in journal form. The sophistication in the writing such as unfamiliar words and extensive use of figurative language made it a harder read for others.\n","\n","[Pride and Prejudice Review]  \n","\n","<img src=https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1320399351i/1885.jpg width=150 height=250>\n","\n","[Pride and Prejudice Review]: https://www.goodreads.com/book/show/1885.Pride_and_Prejudice?ref=nav_sb_ss_1_19\n","\n","There are mostly negative opinions for this book due to the lack of substance throughout the story. The character interactions were fairly shallow and did not add a lot of depth to the overall plot. There were not any initiatives taken by the protagonist to address her shortcomings. While the themes centering around the book are novel and significant, Austen made them quite subtle for the reader to identify. Our group members simply treated this book as \"boring.\"\n","\n","[Dracula Review]\n","\n","<img src=https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1387151694i/17245.jpg width=150 height=250>\n","\n","[Dracula Review]: https://www.goodreads.com/book/show/17245.Dracula?from_search=true&from_srp=true&qid=kdqsk6iihp&rank=1\n","\n","Among us who read the book, we did not like reading Dracula. The story is supposed to be mysterious, but the plot felt too slow to fully immerse ourselves into the story to unravel the mystery of Count Dracula. Some of Count Dracula's questionable actions in his interactions with other characters made it a weird read, and like many classic books, the use of unfamiliar vocabulary and old grammatical structure made it a hard read. Some think the plot was not worth the effort that went into unraveling the language of the book.\n","\n","[The Great Gatsby Review]\n","\n","<img src=https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1490528560i/4671.jpg width=150 height=250>\n","\n","[The Great Gatsby Review]: https://www.goodreads.com/book/show/4671.The_Great_Gatsby?ref=nav_sb_ss_1_16\n","\n","Our group was overall positive reading The Great Gatsby. The themes centering around the Roaring Twenties era are well-thought out and paints a contemporary image of using wealth for your own benefit. The settings and characters are unique and likeable for their role in the story. Fitzgerald cleverly used language that paints a vivid image of the American Dream with the extravagant and bombastic atmosphere. It is overall a classic many deem a great read to explore the culture surrounding the 1920s.\n"],"metadata":{"id":"McbnsHqohk5Q"}},{"cell_type":"markdown","source":["## Metrics  \n","\n","The three metrics we will use to measure the books are: total word count, average word length, and total unique words. These can provide insight as to what makes us like or dislike these books. Total word count indicates the overall length of the book, which can play a role in our likeness when it comes to the pacing of the story. Is it paced just right, or is it too rushed or too slow? The average word length can indicate the level of complexity of the words the author uses throughout the story. The total unique words can indicate the amount of unfamiliar words in the story that could have made it a harder read. In the following section, we will go over each metric and then provide an accompanying figure showcasing the metrics for our respective books. Our overall hypothesis is that our metrics for the books we tend to dislike are higher compared to the books we tend to like. Meaning, the books we dislike will have overall higher word count, longer average word length, and more unique words used throughout."],"metadata":{"id":"CvSCteaOhuDY"}},{"cell_type":"markdown","source":["### Creating Book Files (.txt)"],"metadata":{"id":"IkmK6OCwh_hU"}},{"cell_type":"markdown","source":["#### Romeo and Juliet by William Shakespeare"],"metadata":{"id":"Abzc6B2Z7ReJ"}},{"cell_type":"markdown","source":["Download Romeo and Juliet from Project Gutenberg in Plain Text UTF-8 style using `wget` linux command."],"metadata":{"id":"rulSOK3T7ReK"}},{"cell_type":"code","source":["!wget https://www.gutenberg.org/cache/epub/1513/pg1513.txt"],"metadata":{"id":"hF9Y9CU77ReK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use `ls` to check that Romeo and Juliet has been downlaoded."],"metadata":{"id":"Nw8UgxOd7ReL"}},{"cell_type":"code","source":["!ls"],"metadata":{"id":"_YQfLJc37ReL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using `cat`, view the contents of the file containing the book."],"metadata":{"id":"00nq1zEo7ReM"}},{"cell_type":"code","source":["!cat pg1513.txt"],"metadata":{"id":"VnpeJIEd7ReM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can see that the file contains other text than the book itself. \\*\\** START OF THE PROJECT GUTENBERG EBOOK ROMEO AND JULIET \\*\\** and \"\\*\\** END OF THE PROJECT GUTENBERG EBOOK ROMEO AND JULIET *** mark the start and end of the story. Using `grep` with the additional argument `-n` allows us to locate the line number where these two phrases show up."],"metadata":{"id":"LgqejZ0a7ReM"}},{"cell_type":"code","source":["!grep -n \"*** START OF THE PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***\" pg1513.txt\n","!grep -n \"*** END OF THE PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***\" pg1513.txt"],"metadata":{"id":"_KHWznYu7ReM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using these line numbers, slice the original file and redirect it to a new file that will only contain the story of Romeo and Juliet. Use the `sed` command with `-n` to output only the story (slice lines 23-5289) and use the `>` to redirect the output into a new file called `Romeo_and_Juliet.txt`. We can double check the new file was created with the content we want by using `head` and `tail` which will print the first 10 lines and last 10 lines of the file."],"metadata":{"id":"aacbRLrS7ReN"}},{"cell_type":"code","source":["!sed -n '23,5298p' pg1513.txt > Romeo_and_Juliet.txt\n","!head Romeo_and_Juliet.txt\n","!tail Romeo_and_Juliet.txt"],"metadata":{"id":"IoyxevJh7ReN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, the same process will be used to create .txt files only containing the stories of the other four books (Frankenstein, Pride and Prejudice, Dracula, The Great Gatsby)"],"metadata":{"id":"RRRJZsS47ReN"}},{"cell_type":"markdown","source":["#### Frankenstein by Mary Wollstonecraft Shelley"],"metadata":{"id":"j5JvWysP7ReO"}},{"cell_type":"code","source":["!wget https://www.gutenberg.org/cache/epub/84/pg84.txt"],"metadata":{"id":"os2D2eAc7ReO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!grep -n \"*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\" pg84.txt\n","!grep -n \"*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\" pg84.txt"],"metadata":{"id":"3RAMgIt67ReO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sed -n '25,7392p' pg84.txt > Frankenstein.txt\n","!head Frankenstein.txt\n","!tail Frankenstein.txt"],"metadata":{"id":"REAxZv7a7ReP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Pride and Prejudice by Jane Austen"],"metadata":{"id":"zLhOktrd7ReP"}},{"cell_type":"code","source":["!wget https://www.gutenberg.org/cache/epub/1342/pg1342.txt"],"metadata":{"id":"UKnDT0ZM7ReP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!grep -n \"*** START OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE ***\" pg1342.txt\n","!grep -n \"*** END OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE ***\" pg1342.txt"],"metadata":{"id":"fdxUSkew7ReQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sed -n '24,14561p' pg1342.txt > Pride_and_Prejudice.txt\n","!head Pride_and_Prejudice.txt\n","!tail Pride_and_Prejudice.txt"],"metadata":{"id":"DkyHOKrW7ReQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Dracula by Bram Stoker"],"metadata":{"id":"-t_VQwBc7ReQ"}},{"cell_type":"code","source":["!wget https://www.gutenberg.org/cache/epub/345/pg345.txt"],"metadata":{"id":"Sx0AoV6YjhCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!grep -n \"*** START OF THE PROJECT GUTENBERG EBOOK DRACULA ***\" pg345.txt\n","!grep -n \"*** END OF THE PROJECT GUTENBERG EBOOK DRACULA ***\" pg345.txt"],"metadata":{"id":"WwvqQGdvjhCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sed -n '23,15524p' pg345.txt > Dracula.txt\n","!head Dracula.txt\n","!tail Dracula.txt"],"metadata":{"id":"v4MeWGeCjhCx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### The Great Gatsby by F. Scott Fitzgerald"],"metadata":{"id":"QZfCfM9s7ReR"}},{"cell_type":"code","source":["!wget https://www.gutenberg.org/cache/epub/64317/pg64317.txt"],"metadata":{"id":"MS6Bt-7q7ReR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!grep -n \"*** START OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***\" pg64317.txt\n","!grep -n \"*** END OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***\" pg64317.txt"],"metadata":{"id":"wcinohQI7ReS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sed -n '22,6426p' pg64317.txt > The_Great_Gatsby.txt\n","!head The_Great_Gatsby.txt\n","!tail The_Great_Gatsby.txt"],"metadata":{"id":"GBlXp_Ci7ReS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Total Word Count"],"metadata":{"id":"vJgTwWRs55jb"}},{"cell_type":"markdown","source":["We define a function called `word_count` that takes the parameter of a text. The function utilizes a counter variable `count` to count the number of words in the text. The `for` loop goes through each line of the given text. `line.split()` splits the line into words in which we use the `len()` function to get the number of words in that line. The number of words is added to the counter. Once the `for` loop goes through all the lines in the given text, then the function will output the total number of words."],"metadata":{"id":"LEnX_OEh6U52"}},{"cell_type":"code","source":["def word_count(text):\n","    # Initialize counter\n","    count = 0\n","\n","    # Count the number of words in the text\n","    for line in text:\n","        count += len(line.split())\n","\n","    return count"],"metadata":{"id":"lkWyXLf0_cxg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With the `word_count()` function to count the total number of words in a text, we now apply it to the five books. The files are read using `with open` which removes the need to open and close each text file as we would have to do by only using `open()` when we want to manipulate the file. The `open()` function needs a file as a parameter and `r` is included to specify we want to read the file. The `as` gives the read file an alias to be worked with."],"metadata":{"id":"dL5Vp_Dz7RgL"}},{"cell_type":"markdown","source":["After the file is read, we apply the function to get the total word count for the each of our books."],"metadata":{"id":"RHHG8ql18O5f"}},{"cell_type":"code","source":["with open('Romeo_and_Juliet.txt', 'r') as Romeo_and_Juliet:\n","    RJ_twc = word_count(Romeo_and_Juliet)\n","\n","with open('Frankenstein.txt', 'r') as Frankenstein:\n","    F_twc = word_count(Frankenstein)\n","\n","with open('Pride_and_Prejudice.txt', 'r') as Pride_and_Prejudice:\n","    PP_twc = word_count(Pride_and_Prejudice)\n","\n","with open('Dracula.txt', 'r') as Dracula:\n","    D_twc = word_count(Dracula)\n","\n","with open('The_Great_Gatsby.txt', 'r') as The_Great_Gatsby:\n","    TGG_twc = word_count(The_Great_Gatsby)\n","\n","print(f'The total word count for the following books: \\n1. Romeo and Juliet {RJ_twc} \\n2. Frankenstein {F_twc} \\n3. Pride and Prejudice {PP_twc} \\n4. Dracula {D_twc} \\n5. The Great Gatsby {TGG_twc}')"],"metadata":{"id":"L0V137so8KNe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Average Word Length\n"],"metadata":{"id":"_SWHMZ1X6SZS"}},{"cell_type":"markdown","source":["We define the function `average_word_length` that takes the parameter being the text of our book. It initializes the variables `total_length` and `word_count` which will keep track of the total length of words and the number of words (used as a counter). The first `for` loop splits a line of the text into words. The second `for` loop works off of these words by going through each word and adding the length of the word, found using `len()`, to `total_length` and counting the number of words by adding 1 to `word_count` per word. The average length of the words are found by dividing `total_length` by `word_count` and set to the variable `average_length` which is the final output of the function.\n"],"metadata":{"id":"LLTKBQfYe6wi"}},{"cell_type":"code","source":["def average_word_length(text):\n","  # Initialization of variables\n","  total_length = 0\n","  word_count = 0\n","\n","  for line in text:\n","    words = line.split()\n","    for word in words:\n","      total_length += len(word)\n","      word_count += 1\n","  average_length = total_length/word_count\n","  return average_length"],"metadata":{"id":"bCEm2pCp6Zwi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using the same format as before to read the files, we apply the function to get the average word length for the each of our books."],"metadata":{"id":"tj1qwdNoyF3y"}},{"cell_type":"code","source":["with open('Romeo_and_Juliet.txt', 'r') as Romeo_and_Juliet:\n","    RJ_awl = average_word_length(Romeo_and_Juliet)\n","\n","with open('Frankenstein.txt', 'r') as Frankenstein:\n","    F_awl = average_word_length(Frankenstein)\n","\n","with open('Pride_and_Prejudice.txt', 'r') as Pride_and_Prejudice:\n","    PP_awl = average_word_length(Pride_and_Prejudice)\n","\n","with open('Dracula.txt', 'r') as Dracula:\n","    D_awl = average_word_length(Dracula)\n","\n","with open('The_Great_Gatsby.txt', 'r') as The_Great_Gatsby:\n","    TGG_awl = average_word_length(The_Great_Gatsby)\n","\n","print(f'The average word length for the following books: \\n1. Romeo and Juliet {round(RJ_awl, 3)} \\n2. Frankenstein {round(F_awl, 3)} \\n3. Pride and Prejudice {round(PP_awl, 3)} \\n4. Dracula {round(D_awl, 3)} \\n5. The Great Gatsby {round(TGG_awl, 3)}')"],"metadata":{"id":"4LMkdBKhvH1D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Total Unique Words"],"metadata":{"id":"Wqh1W1lZFCsd"}},{"cell_type":"markdown","source":["We define the function `total_unique_words` that takes the parameter being the text of our book. This can be done by initializing an empty array to store each unique word. To address possible duplicates, we clean the text by using a few methods like setting all words to lower case and removing any punctuation. We create a `for` loop that reads each line where it splits each line into individual words. The condition is if the word is not in the array, it appends to our list. We finally sort the list of unique words for clarity and get the total count of unique words.\n"],"metadata":{"id":"3mKiV3M9fUkw"}},{"cell_type":"code","source":["def total_unique_words(text):\n","  # cleaning\n","  text = text.read()\n","  text = text.lower()\n","  words = text.split()\n","  words = [word.strip('.,!?()[]&*_:;') for word in words]\n","  words = [word.replace(\"'s\", '') for word in words]\n","\n","  unique_list = []\n","  for word in words:\n","    if word not in unique_list:\n","      unique_list.append(word)\n","  unique_list.sort()\n","  total_unique = len(unique_list)\n","  return(unique_list, total_unique)"],"metadata":{"id":"Tca08XOyBm0w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using the same format as before to read the files, we apply the function to get the total unique words for the each of our books. Since there are two outputs for the function `total_unique_words`, we have `x_unique` for the list containing the sorted unique words and `x_unique_total` for the total count of unique words."],"metadata":{"id":"Ziawaumq3Q_I"}},{"cell_type":"code","source":["with open('Romeo_and_Juliet.txt', 'r') as Romeo_and_Juliet:\n","  RJ_unique, RJ_unique_total = total_unique_words(Romeo_and_Juliet)\n","\n","with open('Frankenstein.txt', 'r') as Frankenstein:\n","    F_unique, F_unique_total = total_unique_words(Frankenstein)\n","\n","with open('Pride_and_Prejudice.txt', 'r') as Pride_and_Prejudice:\n","    PP_unique, PP_unique_total = total_unique_words(Pride_and_Prejudice)\n","\n","with open('Dracula.txt', 'r') as Dracula:\n","    D_unique, D_unique_total = total_unique_words(Dracula)\n","\n","with open('The_Great_Gatsby.txt', 'r') as The_Great_Gatsby:\n","    TGG_unique, TGG_unique_total = total_unique_words(The_Great_Gatsby)\n","\n","print(f'The total unique words for the following books: \\n1. Romeo and Juliet {RJ_unique_total} \\n2. Frankenstein {F_unique_total} \\n3. Pride and Prejudice {PP_unique_total} \\n4. Dracula {D_unique_total} \\n5. The Great Gatsby {TGG_unique_total}')"],"metadata":{"id":"WerBmCnl3O1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualizing the Metrics\n"],"metadata":{"id":"ATEMePgtgivA"}},{"cell_type":"markdown","source":["Using the package `matplotlib.pyplot`, visualization of the tested metrics were created."],"metadata":{"id":"-01ClAxVBGkL"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"kNCUhckfBNJF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To visualize the total word count, a bar graph was made to compare the metric amongst the five books. The books that had the most word count are as follows: Dracula, Pride and Prejudice, Frankenstein, The Great Gatsby, and Romeo and Juliet."],"metadata":{"id":"IDu39SOkBQYc"}},{"cell_type":"code","source":["# x-axis\n","books = [\"Romeo and Juliet\", \"Frankenstein\", \"Pride and Prejudice\", \"Dracula\", \"The Great Gatsby\"]\n","# y-axis\n","all_word_counts = [RJ_twc, F_twc, PP_twc, D_twc, TGG_twc]\n","\n","# plotting the data as bar graph\n","plt.bar(books, all_word_counts)\n","plt.xlabel(\"Books\")\n","plt.xticks(rotation=45)\n","plt.ylabel(\"Word Count\")\n","plt.title(\"Word Count of Different Books\")\n","plt.show()"],"metadata":{"id":"DHfbW9X7ytg1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To visualize the average word length, a scatter plot was made to compare the metric amongst the five books. A scatter plot was chosen instead of a bar graph due to the close similarity of the values. The books that had the longest average word length are as follows: Pride and Prejudice, Frankenstein, The Great Gatsby, Romeo and Juliet, and Dracula."],"metadata":{"id":"Zs1S31rBBz_o"}},{"cell_type":"code","source":["# y-axis\n","all_average_word_lengths = [RJ_awl, F_awl, PP_awl, D_awl, TGG_awl]\n","\n","# plotting a scatter plot\n","plt.scatter(books, all_average_word_lengths)\n","plt.xlabel(\"Books\")\n","plt.xticks(rotation=45)\n","plt.ylabel(\"Average Word Length\")\n","plt.title(\"Average Word Length of Different Books\")\n","plt.show()"],"metadata":{"id":"17Aqy5y21xse"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To visualize the total unique word count, a bar graph was made to compare the metric amongst the five books. The books that had the most unique word count are as follows: Dracula, Pride and Prejudice, Frankenstein, The Great Gatsby, and Romeo and Juliet. This follows the same rankings as the total word count metric."],"metadata":{"id":"Wguvo29jCL4K"}},{"cell_type":"code","source":["all_total_unique = [RJ_unique_total, F_unique_total, PP_unique_total, D_unique_total, TGG_unique_total]\n","\n","plt.bar(books, all_total_unique)\n","plt.xlabel(\"Books\")\n","plt.xticks(rotation=45)\n","plt.ylabel(\"Total Unique Words\")\n","plt.title(\"Total Unique Words of Different Books\")\n","plt.show()"],"metadata":{"id":"aUMRsQCA1zX3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conclusion\n","Overall, we found a fairly strong trend in each of the metrics that support our hypothesis. Our two most disliked books being Dracula and Pride and Prejudice topped the charts for highest word count and most unique words. While it is noteworthy to feature an extensive vocabulary by incorporating more words, it makes the reading more challenging to follow especially for readers having to read the same paragraph multiple times due to the unfamiliar vocabulary. We also found that they have the highest word count which contributes to our disliking because they both have a rather shallow plot, and by stretching the word count makes the flow overall slow. Both The Great Gatsby and Frankenstein reside in the middle of the stack. This suggests they have the right amount in terms of word count, word length, and number of unique words. Having the right balance of these metrics can enhance the reading experience which most of us agreed on for the respective books."],"metadata":{"id":"T9Fz5XEDdVF4"}},{"cell_type":"markdown","source":["# Question 3\n"],"metadata":{"id":"Ea6hPztESLpK"}},{"cell_type":"markdown","source":["### The issue of our generation is anthropogenic induced climate change. A world in a state of perpetual climatic instability sets the seed for a world devoid of life itself."],"metadata":{"id":"_4WpSTv5yqfU"}},{"cell_type":"markdown","source":["\n","In order to preserve what life still exists, we must harness all of the tools at our disposal, the most important of which is data.\n","\n","Science is driven by observation. In essence, observation leads to analysis, driving scientific discovery. The more data that exists for any given study, the higher its potential.\n","\n","AWS has many open datasets encompassing information from seemingly endless disciplines. Nonetheless, a common trend remains when navigating the database: the prevalence of datasets geared towards academic and scientific research.\n","\n","### The long-term surival of our species hinges on our ability to leverage ecological and climate-focused databases to engineer a sustainable future. For these reasons, we decided to examine the following three databases through AWS:\n","\n","#### 1) UCSC Genome Browser Sequence and Annotations\n","\n","#### 2) NOAA U.S. Climate Normals\n","\n","#### 3) Sea Around Us Global Fisheries Catch Data"],"metadata":{"id":"V9SVRf1Ytpbn"}},{"cell_type":"markdown","source":["## 1) UCSC Genome Browser Sequence and Annotations\n","\n"],"metadata":{"id":"sqtl76DcTV4K"}},{"cell_type":"markdown","source":["We examined the UCSC Genome Browser Sequence and Annotations, which is a large bioinformatic database encompassing the entire human genome, as well as many other model genetic system genomes. This database contains DNA sequence information, repeated elements, regulatory RNA sequences, and distinguishable variants between humans and other orthologous species.\n","\n","In terms of climactic and ecological importance, the majority of DNA is conserved between species, with levels reaching over 99% between species of the same kingdom. Understanding the full genetic code can help to splice genetic diversity back into species threatened by anthropogenic induced change."],"metadata":{"id":"4d6-pgdmwzYM"}},{"cell_type":"markdown","source":["In order to access AWS commands, the following package is needed:"],"metadata":{"id":"M43VslOGUSSE"}},{"cell_type":"code","source":["!pip install awscli"],"metadata":{"id":"0cRHbUquUPGB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We first called in the genome browser data\n","\n"],"metadata":{"id":"vUnUSYz8Ujo4"}},{"cell_type":"code","source":["!aws s3 ls --no-sign-request s3://genome-browser/"],"metadata":{"id":"UDK0__YmUmbq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This output represents multiple prefixes, all which contain data related to different disciplines of bioinformatics. We chose goldenPath, which includes genetic sequence alignment data.\n","\n","After choosing this, we use a request to Amazon S3 to get the list of objects in our bucket of choice (the goldenPath within the genome browser). We used the following code: `!aws s3 ls --no-sign-request --recursive s3://genome-browser/goldenPath/`. It took a long time to run, but we were able to sift through the list and decide which object to use."],"metadata":{"id":"hraTqbuHUnFz"}},{"cell_type":"markdown","source":["We chose to examine the human Y-chromosome data, and used a call below to save the Y-chromosome data as a folder. We then attempted to visualize some of the data to better understand it."],"metadata":{"id":"xKwrFfQBWSA2"}},{"cell_type":"code","source":["!aws s3 cp --no-sign-request s3://genome-browser/goldenPath/10april2003/chromosomes/chrY.fa.zip ./Chromosome_Y_Data.csv\n"],"metadata":{"id":"w6eLstYRXy2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","Y_File = pd.read_csv('Chromosome_Y_Data.csv')\n","\n","print(Y_File.head())"],"metadata":{"id":"RvHx5-ODq__j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unfortunately, we were unable to decode the data type, as it did not compute with the UTF-8 encoding used in Python. This is likely due to the abnormal nature of bioinformatic data. We elected not to examine the UCSC gene browser for further analysis due to the sheer size of the dataset, the lack of tutorials and difficulty in understanding the encoding."],"metadata":{"id":"H-xmFhZpWPgf"}},{"cell_type":"markdown","source":["## 2) NOAA U.S. Climate Normals"],"metadata":{"id":"aQeNoFmye4FP"}},{"cell_type":"markdown","source":["The NOAA U.S. Climate Normals is a dataset that shows normal climate conditions throughout the U.S. This includes temperature, snow, precipitation, and elevation in various areas.\n","This dataset is vitally important to climate change, as it gives baseline climate data from the 1950's until now (by decade). Trends in this dataset can be used to predict the extent of future climate change."],"metadata":{"id":"antxN3JTfFIj"}},{"cell_type":"markdown","source":["We first called in the dataset with AWS functions"],"metadata":{"id":"ofTL1zTFmyR9"}},{"cell_type":"code","source":["!aws s3 ls --no-sign-request s3://noaa-normals-pds/"],"metadata":{"id":"7d4JTVDUm7-Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The outcomes contained four prefixes representing different intervals of data collection:\n","\n","\n","1.   Annual Seasonal\n","2.   Daily\n","3.   Hourly\n","4.   Monthly\n","\n"],"metadata":{"id":"rIJ6ypJ_nCc8"}},{"cell_type":"markdown","source":["We chose the Annual Seasonal prefix, as it gives a broader scope over the course of decades.\n","\n","After choosing this, we use a request to Amazon S3 to get the list of objects in our bucket of choice (the Annual Seasonal data within the NOAA U.S. Climate Data)."],"metadata":{"id":"IAu6bXvIo8wm"}},{"cell_type":"code","source":["!aws s3 ls --no-sign-request --recursive s3://noaa-normals-pds/normals-annualseasonal/"],"metadata":{"id":"LQi7NGjc0WBT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We chose to examine the USC00296911 object, and used a call below to save the it data as a folder. We then attempted to visualize some of the data to better understand it."],"metadata":{"id":"gZkDATZ0opwy"}},{"cell_type":"code","source":["!aws s3 cp --no-sign-request s3://noaa-normals-pds/normals-annualseasonal/access/USC00296911.csv ./Season_File"],"metadata":{"id":"-9xT7JqTxrPr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After sending the contents of the Annual Seasonal data to a new file (Season_File), we converted it to a pandas csv and displayed the first five rows of the csv."],"metadata":{"id":"_zdsgxqOp3-G"}},{"cell_type":"code","source":["import pandas as pd\n","\n","NOAA_File = pd.read_csv('Season_File')\n","\n","print(NOAA_File.head())"],"metadata":{"id":"7vf_nW59kosy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unfortunately, much of the data was truncated in the output, so we needed to examine the data further"],"metadata":{"id":"XLrq2zbUp4PE"}},{"cell_type":"code","source":["NOAA_File.dtypes"],"metadata":{"id":"KkaCSI5QkrnC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This showed that there were 567 entries for data columns within this csv."],"metadata":{"id":"3qX179Hjp5bS"}},{"cell_type":"code","source":["NOAA_data_dict = NOAA_File.dtypes.to_dict()\n","for column, dtype in NOAA_data_dict.items():\n","    print(f\"{column}: {dtype}\")"],"metadata":{"id":"DAR-0ovMkvFn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We then converted the output of the datatypes to a dictionary, and examined how many key value pairs were in the dictionary, and what data types they were.\n","\n","Unfortunately, there were far too many data entries within this data set, and the abbreviations were unclear. We had difficulty determining what each data column label meant, and how to determine useful correlations and trends between them."],"metadata":{"id":"3lYpRpKYqY46"}},{"cell_type":"markdown","source":["## 3) Sea Around Us Global Fisheries Catch Data\n"],"metadata":{"id":"MLYOtLZ2WBUx"}},{"cell_type":"markdown","source":["The sea around us global fisheries catch data is aggregate of fishery and catch statistics. This data includes the recorded scientific and common names, the total mass, commercial groups and entities, the catch status, gear and year of catch. This dataset has the potential to show important trends in relation to marine biodiversity loss and global climate change."],"metadata":{"id":"u_gPBuZwZhZS"}},{"cell_type":"markdown","source":["We started by calling the open dataset with AWS on google colab\n"],"metadata":{"id":"cKwdUdfxa5_A"}},{"cell_type":"code","source":["!aws s3 ls --no-sign-request s3://fisheries-catch-data/"],"metadata":{"id":"3Trv8DCsa9Wh","cellView":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This output represents two prefixes, one with spatial data and the other with catch data. We chose to use the catch data."],"metadata":{"id":"OwV3hrcmbMeC"}},{"cell_type":"code","source":["!aws s3 ls --no-sign-request --recursive s3://fisheries-catch-data/global-catch-data/"],"metadata":{"id":"0lGv5hnY-OJC","cellView":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There is only one folder with information, with a total of 203326590 bytes. We then downloaded the information from the S3 Bucket and saved it as \"Fish_File\" in our current working directory."],"metadata":{"id":"jcZKjxX6bM9J"}},{"cell_type":"code","source":["!aws s3 cp --no-sign-request s3://fisheries-catch-data/global-catch-data/csv/rfmo_12.csv ./Fish_File"],"metadata":{"id":"FGjjFYllbEPf","cellView":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After sending the contents of the global catch data to a new file (Fish_File), we convert it to a pandas csv and display the first five rows of the csv.\n","\n"],"metadata":{"id":"v_FGEK86bNUJ"}},{"cell_type":"code","source":["import pandas as pd\n","\n","Fish_File = pd.read_csv('Fish_File')\n","\n","print(Fish_File.head())"],"metadata":{"id":"FsP3RVANd6Sn","cellView":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to create visualizations, we needed to examine the datatypes."],"metadata":{"id":"Ggrp5wcMd_fq"}},{"cell_type":"code","source":["Fish_File.dtypes"],"metadata":{"id":"VvGVIyRZbF_D","cellView":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We decided that the most informative data subset would be the catch sum, and decided to visualize it through a histogram. However, using the entire dataset took a large amount of time, so we subsetted the data further to only visualize a random 1% of the data."],"metadata":{"id":"DkqQ9aU5Hqan"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","Fish_File_Subset = Fish_File.sample(frac = 0.01).copy()\n","plt.scatter(Fish_File_Subset['year'].values, Fish_File_Subset['catch_sum'].values, s = 5)"],"metadata":{"id":"PEtMrw6ZFLU9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Skt2EJio2oNG"},"source":["# Question 4\n","## Data walkthrough of Sea Around Us Global Fisheries Catch Data\n","\n","The question we are interested in answering is \"which fishing entities earn the most from fishing?\"\n","<br><br>\n","Before answering the question of earnings, the data must be cleaned. Feature engineering and domain expertise can be used to most effectively represent the data and handle invalid data. However, for this simple analysis, the data will not be processed besides deletion of invalid data.\n","<br><br>\n","To begin, the Sea Around Us Global Fisheries Catch Data can be downloaded from an AWS S3 bucket using the commands below:"]},{"cell_type":"code","source":["%%bash\n","pip install awscli pandas matplotlib\n","aws s3 cp --no-sign-request s3://fisheries-catch-data/global-catch-data/csv/rfmo_12.csv ./Fish_File"],"metadata":{"id":"YD0O0APuYZM1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data is read into a Pandas Dataframe object. After creating a database object, a high-level overview can be found with `df.info()`. This function shows column names, total data size, non-null values for each column, and the data type of each column."],"metadata":{"id":"Bf5slLsVZM3E"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"HeEdthvw4SD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"Fish_File\")\n","df.info()"],"metadata":{"id":"59Ahni2k6Jq_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From first glance, it appears that there are no null values and that there are only 4 numerical data types. However, further analysis needs to be done to identify data types because categorical data could be represented numerically. Additionally, null-values can be represented with negative values for numeric data or a string saying \"none\" for string data.\n","<br>\n","<br>\n","Below is a description of each column obtained from Sea Around Us."],"metadata":{"id":"NkhiUjFPZxQm"}},{"cell_type":"markdown","source":["### Summary of data in Fisheries Catch Data\n","\n","| Column Name | Data Type | Description/ Comment |\n","| --- | --- | --- |\n","|rfmo_id|int64|RFMO identifier|\n","|rfmo_name|string|RFMO short name|\n","|layer_name|string|Catch data layer name|\n","|year|int64|Year of the catch|\n","|scientific_name|string|Current valid scientific name from FishBase or SeaLifeBase|\n","|common_name|string|The English common name from FAO ASFIS Species List|\n","|functional_group|string|Ecological functional group description|\n","|commercial_group|string|Commercial group name|\n","|fishing_entity|string|Fishing entity name|\n","|sector_type|string|Fishery Sector type|\n","|catch_status|string|If catch is landed or discarded|\n","|reporting_status|string|If catch is reported or unreported|\n","|gear_name|string|Fishing gear name|\n","|catch_sum|float64|Weight of the catch in metric tons|\n","|real_value|float64|Financial value of the catch|\n","\n","Source: https://github.com/SeaAroundUs/sau-ubc-cic-catchdata/blob/main/data-dictionary/web_v_fact_data.md"],"metadata":{"id":"9vtwA0ZmXmqa"}},{"cell_type":"markdown","source":["The function `df.head()` is used to look at a small subset of data."],"metadata":{"id":"3b96UpZgai2X"}},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"ljESgLLhXcjo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function `df.select_dtypes()` is used to divide the dataset by numerical and categorical data types for univariate analysis."],"metadata":{"id":"I08WsoAS2A3t"}},{"cell_type":"code","source":["numerical_df = df.select_dtypes(include=\"number\")\n","categorical_df = df.select_dtypes(\"object\")"],"metadata":{"id":"qMB2ASjTXrbb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function `df.value_counts()` provides a series with the frequency of each class for a column. Only classes with a frequency greater than 10000 were printed since some columns have many classes with frequencies less than 10000. From this view, we see that:\n","- `rfmo_name` does not provide useful information since it only has one class.\n","- `layer_name` does not provide useful information since it only has one class\n","- `scientific_name` has a null value represented by \"Marine fishes not identified\"\n","- `gear_name` has a null value represented by \"unknown\" class\n","\n","From these findings, `rfmo_name` and `layer_name` will not be used. The null values from `layer_name` and `scientific_name` will be removed"],"metadata":{"id":"3MqqZmiH2Qik"}},{"cell_type":"code","source":["for column in categorical_df:\n","  series = categorical_df[column].value_counts()\n","  series = series[series.gt(10000)]\n","  print(\"-------------------------------------------------------\")\n","  print(series)\n"],"metadata":{"id":"-yt3ligp0zkP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function `DataFrame.describe()` can be used to get a high-level summary of the numerical values. The transpose option `.T` is used to present the data horizontally. From this view, we see that:\n","- `rfmo_id` does not provide useful information since it is only one value.\n","- `catch_sum` has a minimum of 0.\n","- `real_value` has values greater than 0 and less than 1."],"metadata":{"id":"T6_Zq_o3RAl3"}},{"cell_type":"code","source":["numerical_df.describe().T"],"metadata":{"id":"44oZ5ZZB4Bre"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["While `catch_sum` represents the weight of the catched fish, it is possible for the data to be 0 because the units are in metric tons with one significant figure.\n","<br><br>\n","The `real_value` column can have small values since it is possible to not have high profits selling fish."],"metadata":{"id":"hdyYSLsZSXjk"}},{"cell_type":"markdown","source":["From these findings, `rfmo_name` will not be used. The low number values from `catch_sum` and `real_value` will not be removed."],"metadata":{"id":"JpZOSp1mSi0a"}},{"cell_type":"markdown","source":["### Cleaning data\n","As discussed, the columns `rfmo_name`, `layer_name`, and `rfmo_id` will be removed using the `DataFram.drop()` function. And the invalid values from `scientific_name` and `gear_name` are removed with boolean expressions."],"metadata":{"id":"w_PvupuNSxy0"}},{"cell_type":"code","source":["df = df.drop(labels=[\"rfmo_name\", \"layer_name\", \"rfmo_id\"], axis=1)\n","df = df[df[\"scientific_name\"] != \"Marine fishes not identified\"]\n","df = df[df[\"gear_name\"] != \"unknown\"]\n","df.info()"],"metadata":{"id":"Y23FNaWGTPZp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Which fishing entities earn the most from fishing?\n","\n","The columns of interest are `fishing_entity` and `real_value`. The `real_value` column needs to be averaged to accurately represent the earnings per entity.\n","<br><br>\n","Pandas allows averaging groups of data using the commands `df.groupby(\"category\")[\"value\"].mean()`.\n","<br><br>\n","The output Series is formatted such that:\n","- it is in descending order `Series.sort_values(ascending=False)`\n","- only categories with an average greater than 10^7 are shown `Series.gt(VAL)`"],"metadata":{"id":"ROBkvVtZevt0"}},{"cell_type":"code","source":["average_values = df.groupby(\"fishing_entity\")[\"real_value\"].mean()\n","average_values = average_values.sort_values(ascending=False)\n","average_values = average_values[average_values.gt(10000000)]"],"metadata":{"id":"8F8ZG7nMdogw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(facecolor=\"white\")\n","sns.barplot(x=average_values.index, y=average_values.values)\n","fig.suptitle(f'Average Real Value Earned by Fishing Entity')\n","ax.set_xlabel('Fishing Entity')\n","ax.set_ylabel('Average Real Value Earned (10^8 USD)')\n","for tick in ax.get_xticklabels():\n","    tick.set_rotation(90)"],"metadata":{"id":"tHs7vTCddqs8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Conclusion\n","The fishing entities that earned the most from fishing are Norway, Russian Federation, Iceland, Lithuania, and Netherlands. Both Norway and Russian Federation earned an average of over $200,000,000 USD from fishing. A future topic of interest is adding a third dimension of time to see trends in earnings."],"metadata":{"id":"7ViaeiU0myHz"}},{"cell_type":"markdown","metadata":{"id":"SCKWrIAM_IXR"},"source":["# Question 5\n"]},{"cell_type":"markdown","source":["To begin our search for a Zipf distribution, we will look into the numerical data, which includes `year`, `catch_sum`, and `real_value`."],"metadata":{"id":"mfl6RKQGw_B9"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"metadata":{"id":"OP7M4KUklbST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following cell checks whether `empiricaldist` is installed and installs it if necessary."],"metadata":{"id":"-JLUoc0Ar2wA"}},{"cell_type":"code","source":["try:\n","    import empiricaldist\n","except ImportError:\n","    !pip install empiricaldist"],"metadata":{"id":"yuaSYIveryD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from empiricaldist import Pmf"],"metadata":{"id":"qd0na49ksEid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['year'].value_counts().sort_index()"],"metadata":{"id":"qiTABo12nRgp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The result from `value_counts` is a set of possible values and the number of times each one appears, so it is a kind of distribution. Let's try visualizing it."],"metadata":{"id":"ukW4rhcx4jnc"}},{"cell_type":"code","source":["year = df['year']\n","year.hist(grid=False)\n","plt.xlabel('Year')\n","plt.ylabel('Number of observations')\n","plt.title('Histogram of years in the data')"],"metadata":{"id":"FrWy-Fks4sjq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This doesn't look like a Zipf distribution..."],"metadata":{"id":"dCtRccAy6hj7"}},{"cell_type":"code","source":["pmf_year = Pmf.from_seq(year)\n","pmf_year.bar(label='YEAR')"],"metadata":{"id":"PS2ml1TGs7jt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This looks more like a linear or exponential distribution.\n","\n","Next, we'll evaluate `catch_sum`."],"metadata":{"id":"_x2AkPMLl1Ci"}},{"cell_type":"code","source":["df['catch_sum'].value_counts().sort_index()"],"metadata":{"id":"tZo09PU57Juk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["csum = df['catch_sum']\n","csum.hist(grid=False)\n","plt.xlabel('Weight of Catch (metric tons)')\n","plt.ylabel('Number of observations')\n","plt.title('Histogram of Catch Weight')"],"metadata":{"id":"7WDHwL8S630w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data looks like it is very skewed. Maybe we can get a better picture of the data if we filter out values above 30."],"metadata":{"id":"7UHUqdeQl-za"}},{"cell_type":"code","source":["c_sum = df[df['catch_sum'] <= 30]['catch_sum']\n","c_sum.hist(grid=False)\n","plt.xlabel('Weight of Catch (metric tons)')\n","plt.ylabel('Number of observations')\n","plt.title('Histogram of Catch Weight')"],"metadata":{"id":"_M45U_Es7bcm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This could be a Zipf distribution. If so, it's very skewed."],"metadata":{"id":"lg8kemZYptSB"}}]}